{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surrounded-ocean",
   "metadata": {},
   "source": [
    "# Inference with Models\n",
    "\n",
    "An X-ray transient monitor has detected a new source, also observed by LIGO. This is exciting! However, we don't know what it is, so we'd like to know what Fermi observed. For the purpose of this toy problem, we are going to ignore the question of whether Fermi observed the source at all or not (although that's an important problem, too!). For the purpose of this tutorial, we'll assume we know that Fermi observed something significant! Hooray!\n",
    "\n",
    "What we'd like to know: what are the details of the source that Fermi observed? We'd like to do a multi-wavelength Spectral Energy Distribution, so knowing the total flux that Fermi observed would be very useful to help piece that together. We'd also like to know the precise position of the source. \n",
    "\n",
    "The purpose of the following analysis is to infer the physical properties (its flux and its position) so that we can use these to help us understand the nature of the new, exciting source we've observed. \n",
    "\n",
    "**Important**: This tutorial is based on a toy problem, and while it pretends to be using Fermi, it's not really using Fermi. Any Fermi-related things are entirely made up, and should not be used for actual analysis! This is a toy problem designed to highlight the statistical concepts, and consequently strips out a whole range of important calibration issues for the sake of simplicity and clarity (and also, I don't actually know that much about the Fermi/LAT). \n",
    "\n",
    "## Imports\n",
    "\n",
    "Let's start by importing some libraries that will definitely come in handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I like using the plotting library seaborn to set style defaults\n",
    "# If you don't have seaborn or don't want to use it, comment out the \n",
    "# two lines below\n",
    "import seaborn as snsl\n",
    "sns.set_style('white')\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-class",
   "metadata": {},
   "source": [
    "There are three text files with image data in the folder. \n",
    "\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Load the one that corresponds to your group number (you can use `numpy.loadtxt` or `pandas.read_csv` or whatever else you'd like):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-neighbor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "devoted-martial",
   "metadata": {},
   "source": [
    "The data file contains an image, which is 300 by 300 pixels wide. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-coupon",
   "metadata": {},
   "source": [
    "We can generate x and y coordinates for these using `numpy.meshgrid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 300\n",
    "x, y = np.meshgrid(np.linspace(0,img_size, num=img_size),\n",
    "                   np.linspace(0,img_size,num=img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-russia",
   "metadata": {},
   "source": [
    "### Writing Down a Model\n",
    "\n",
    "First, we're going to need some kind of model to represent that data! Usually, we need something like a Point Spread Function (PSF) that describes how photons get distributed onto bins (or pixels, or whatever your detector has).\n",
    "\n",
    "Let's assume that the Fermi documentation tells us that the Fermi PSF is approximately Gaussian (I don't know that it is, and I kind of doubt it, so make sure to check for applications that are not this toy problem!). I've written down a 2D Gaussian for you below for your convenience. You might want to check that it doesn't include any bugs, though. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_psf(x, y, amp, mean1, mean2, std, bkg):\n",
    "    \"\"\"\n",
    "    Gaussian Point Spread Function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y: (300,300)-size arrays for the x and y coordinates\n",
    "    \n",
    "    amp: float\n",
    "        The amplitude of the Gaussian. Because the Gaussian *integrates* to 1, \n",
    "        this corresponds to the total flux\n",
    "        \n",
    "    mean1, mean2 : float, float\n",
    "        The x- and y-positions of the peak of the Gaussian\n",
    "    \n",
    "    std : float\n",
    "        The width (standard deviation) of the Gaussian\n",
    "        \n",
    "    bkg : float\n",
    "        A constant describing the background noise level.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gpsf : (300,300) array\n",
    "        An array with the model flux values as x and y positions. \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    norm_term = amp / (2. * np.pi * std**2.)\n",
    "    exp_term = np.exp(-((x - mean1)**2. / (2. * std**2.) + (y - mean2)**2. / (2. * std**2.)))\n",
    "    gpsf = norm_term * exp_term + bkg\n",
    "    return gpsf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-cisco",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Guess some reasonable parameters that might allow you to generate model fluxes similar to those in the data you loaded. Then plot both the data and the model fluxes next to each other and compare. How close did you get with the parameters you guessed?\n",
    "\n",
    "Hint: if it's too hard to compare the two images, you can also take a vertical or horizontal slice out of the image and plot the data and model for those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-politics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-veteran",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-donna",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "technical-hearing",
   "metadata": {},
   "source": [
    "So now we have a model with some parameters, and some data. How do we bring those two together? \n",
    "\n",
    "## Writing Down a Likelihood\n",
    "\n",
    "As we've learned in the lecture, this is what a likelihood is for! The likelihood describes how to relate the process we believe generated the emission (here, the flux produced by the source) and the process that modified the emission before it was recorded in the telescope (here, the point spread function) connect to the data recording process (here, we recorded individual photons).\n",
    "\n",
    "**Exercise**: Can you guess from the data (and from what you know about Fermi data), what kind of likelihood might be appropriate here? Discuss with your group (solution below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-threshold",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-hurricane",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-college",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-netherlands",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "prostate-reasoning",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "You think this data probably ought to be modelled with a Poisson distribution, but a collaborator tells you that the Poisson distribution is expensive to compute, and it probably won't make much of a difference, so just use a Gaussian likelihood instead.\n",
    "\n",
    "The Gaussian likelihood is defined based on the normal distribution (unsurprisingly), so that the observed value $c$ in each pixel is picked from the following distribution:\n",
    "\n",
    "$$\n",
    "P(c | \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(\\frac{(c - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the expected flux and $\\sigma$ the uncertainty. For the data we have, you can assume that $\\sigma \\approx \\sqrt{c}$.\n",
    "\n",
    "The function we are interested in is the *likelihood* of the data. The likelihood describes the probability of observing the data we've measured, conditioned on a *physical* model (your surface brightness model) and a *statistical* model (the Gaussian distribution). The physical model describes what you expect your image to look like if there was no noise. The statistical model describes the uncertainty in the measurements because there is noise present. \n",
    "The likelihood function is the product of the probabilities of all pixels, which is the probability of observing a number of counts in a pixel, $c_i$ under the assumption of a model count rate in that same pixel, $\\mu_i$, multiplied together for all $N$ pixels.\n",
    "\n",
    "All in all, the Gaussian likelihood for a given physical model $m(\\mathbf{\\theta})$, which depends on a set of $K$ parameters $\\mathbf{\\theta} = \\{\\theta_1, \\theta_2, ... , \\theta_k\\}$ looks like this:\n",
    "\n",
    "$$L(\\mathbf{\\theta}) = P(\\mathbf{c}|\\mathbf{\\theta}, H) = \\prod_{i=0}^N{\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left(-\\frac{(c - \\mu)^2}{2\\sigma^2}\\right)}$$\n",
    "\n",
    "In practical terms, the likelihood is often very large or very small, and it is more convenient to work with the logarithm of the likelihood:\n",
    "\n",
    "$$ \n",
    "\\log{(L(\\mathbf{\\theta}))} = \\sum_{i=0}^{N}{-\\frac{1}{2}\\log{(2\\pi\\sigma^2)} - \\frac{(c - \\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "\n",
    "**Exercise**: Complete the likelihood function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "severe-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmin = 1000000000000.\n",
    "\n",
    "def loglikelihood(pars, x, y, counts):\n",
    "    \"\"\"\n",
    "    A Gaussian log-likelihood.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pars : iterable\n",
    "        A list or array of parameters\n",
    "        \n",
    "    x, y: (300,300) array\n",
    "        arrays for the x- and y-coordinate\n",
    "    \n",
    "    counts : (300,300) array\n",
    "        An array with count values\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ## call the model function (i.e. the gaussian PSF), use parameter set pars and\n",
    "    ## the coordinate grids to compute the model image\n",
    "\n",
    "    \n",
    "    ## compute Gaussian likelihood\n",
    "\n",
    "    \n",
    "    \n",
    "    ## deal with NaN and inf values of the log-likelihood\n",
    "    ## if the likelihood is either NaN or inf, choose a ridiculously small value\n",
    "    ## to make the model *really* unlikely, but not infinite\n",
    "    if np.isnan(loglike):\n",
    "        loglike = -logmin\n",
    "    elif loglike == np.inf:\n",
    "        loglike = -logmin\n",
    "\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-ranch",
   "metadata": {},
   "source": [
    "Write down the parameters you guessed above to generate your example model fluxes, and then calculate the log-likelihood of these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pars = # add parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood(test_pars, x, y, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-married",
   "metadata": {},
   "source": [
    "## Estimating the \"Best\" Parameters\n",
    "\n",
    "The likelihood gives us an estimate for the probability of having recorded our observing data given some underlying \"true\" model and model parameters. However, we don't know what these true parameters are; that's exactly what we're trying to estimate! We can do so by stepping through different parameter values, and recording the likelihood of having observed the data, given the assumption that these parameters are true. We then pick the parameters that give us the highest likelihood value as the \"best\" parameters given our observed data. This is called **maximum likelihood estimation**. \n",
    "\n",
    "It is a somewhat subtle, but important distinction that the likelihood is *not* a probability distribution of the parameters, but of the data. It assumes that we are drawing random realizations from an underlying model, given a model and parameters that are known to be true. While you can make meaningful statistical inferences about a scientific problem this way, there are some cases where the structure of the likelihood as a probability of the data makes your life a bit harder. Later, we will talk about how Bayes' theorem gives you a convenient way to flip the probabilities around, and get a probability distribution of the parameters given an observed data set. \n",
    "[This paper]() by David W. Hogg gives a great intuitive introduction to the topic of why this matters.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "\n",
    "So how do we search for the maximum likelihood in practice? One thing you can do is make a grid of parameters, calculate the likelihood for each one, and then simply pick the largest one. This can work great, but also has some significant drawbacks: for example, this might work great when you have only one or two parameters to estimate. Once you have more than that, however, this becomes very unwieldy and computationally expensive, because you have to generate a grid over as many dimensions as you have parameters. Secondly, what should the resolution of your grid be? What if you have a likelihood that's zero almost everywhere except for a small region of parameter space? If you make your grid too coarse, you run the risk of never actually getting close to the maximum likelihood at all. If you make it fine-grained, you are wasting a lot of computational time on regions of parameter space that are entirely uninteresting. \n",
    "\n",
    "One common way is to use **optimization**. Optimization algorithms usually start at a single point in parameter space, and then try to find intelligent ways to pick the next step where to estimate the likelihood. Often, this involves calculating the *gradient* of the likelihood to inform the algorithm where to step next. There's a wide range of different algorithms out there, suited to a number of different problems, but one good place to start is the `scipy.optimize` sub-module. \n",
    "\n",
    "One subtlety about optimization is that while we are trying to find the parameters where the likelihood has its *maximum*, most optimization algorithms are in practice *minimization* routines. That is, they attempt to find the minimum of a function. \n",
    "\n",
    "In practice, this is easily dealt with by defining the inverse of the log-likelihood, and then using this in our minimization algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "negloglike = lambda pars, x, y, poisson_flux: -loglikelihood(pars, x, y, poisson_flux)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-moscow",
   "metadata": {},
   "source": [
    "One crucial component of optimization is picking good starting parameters. Bad starting parameters can get you stuck in local optima, that can be far away from the true optimum. The problem is that you don't know what the true optimum is, so you can't really know whether you got stuck in a local optimum. One thing you can do is start the optimization multiple times from different starting parameters, and then pick the outcome that has the highest value for the maximum likelihood. However, for some problems it's also possible to pick reasonably good starting parameters from the data itself.\n",
    "\n",
    "**Exercise**: Can you think of ideas for how to pick starting parameters for the optimization? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-pitch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-clone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-condition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ahead-maintenance",
   "metadata": {},
   "source": [
    "Let's pull all of these together into a single set of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pars = # add your starting parameters here\n",
    "\n",
    "print(start_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-profession",
   "metadata": {},
   "source": [
    "Okay, now we've got all of our starting parameters. \n",
    "\n",
    "Let's run `scipy.optimize.minimze` to see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scipy.optimize.minimize(negloglike, start_pars, args=(x, y, counts), method='bfgs', tol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-anxiety",
   "metadata": {},
   "source": [
    "The output of `scipy.optimize.minimize` is an object with a wealth of useful information. \n",
    "\n",
    "Here are some important ones.\n",
    "\n",
    "You'll want to pay attention to the attributes `success` and `message`, which give you information about whether the algorithm thinks it's succeeded to find the minimum of the function, and gives you some information (though it's usually pretty arcane), why it did or did not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.success)\n",
    "print(res.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-trash",
   "metadata": {},
   "source": [
    "In our case, the algorithm terminated successfully. Hooray!\n",
    "When this is not the case, the error messages can usually be pretty arcane and hard to debug. It's then worth hunting around the internet for help on what they mean and what the issues might be.\n",
    "\n",
    "The `fun` attribute gives you the function value at termination. This is the maximum likelihood we're looking for (or at least its inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum likelihood: \" + str(-res.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-bankruptcy",
   "metadata": {},
   "source": [
    "The other useful information you'll want is stored in the `x` attribute, which has the parameters at which the optimizer has found the maximum likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best-fit parameters: \" + str(res.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-degree",
   "metadata": {},
   "source": [
    "**Exercise**: Compute a model image for the best-fit model parameters, then compare the model to your data. You can compare the images again side by side, or you can plot slices through the image and model directly in one plot (the latter is probably more useful in comparing them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-taylor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-turkish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "frank-crack",
   "metadata": {},
   "source": [
    "**Exercise at home**: `scipy.optimize.minimize` has a number of available algorithms. Take a look at the documentation and explore 2-3 algorithms. Do all of them give you the same output? Do you get noticeably different parameters? Do you think they are better or worse than the standard BFGS algorithm we've chosen above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-classification",
   "metadata": {},
   "source": [
    "### Parameter Uncertainties\n",
    "\n",
    "We'd also like to know the uncertainties on our parameters. One estimate you can gain from the optimization process is to use the *inverse Hessian*. Many algorithms use the first and second derivatives of the likelihood as information on where to step next. We can use these estimates to calculate a measure of the width of the likelihood, which in turn can be interpreted as uncertainties on the parameters. \n",
    "\n",
    "**Note**: Not all minimization algorithms return the *inverse Hessian*, which you can interpret as a covariance matrix. If you are using an algorithm that doesn't, you can still often calculate a numerical approximation (there's a routine e.g. in the `statsmodels` package). \n",
    "\n",
    "**Important**: The calculation of uncertainties this way makes strong assumptions about the shape of the likelihood, which may or may not be appropriate for your problem. In particular, it makes the assumption that the likelihood is approximately Gaussian in shape (and so your uncertainties are symmetric). Some problems, especially those with complex physical models, may have more complicated shapes for the likelihood, or a likelihood that is asymmetric. In this case, the uncertainties derived in this way are not reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameter uncertainty estimates: \" + str(np.sqrt(np.diag(res.hess_inv))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-sector",
   "metadata": {},
   "source": [
    "## Priors\n",
    "\n",
    "Prior probability distributions describe the information we have about our problem and parameters before we've had a chance to look at the data. They usually describe probability distributions over parameters that encode information from the literature, or just known physics (for example, when fitting the optical spectrum of a sun-like star, we can automatically exclude certain temperatures from the prior, below which hydrogen would not fuse to helium, and which therefore are not physically meaningful temperatures). \n",
    "\n",
    "Let's assume that the initial transient detection came from the LIGO gravitational wave observatory. This is exciting, because electromagnetic counterparts to gravitational wave sources are scientifically very interesting. The LIGO observatory can't locate sources very well: their detection maps often stretch across a big stripe of the sky. However, they do give us *some* information about where the source we are interested in should be located on the sky.\n",
    "\n",
    "For the purpose of this toy exercise, we're going to assume the following position estimates from LIGO:\n",
    "x = 150 +/- 20\n",
    "y = 180 +/- 100\n",
    "\n",
    "We also have some information about the background and the Fermi PSF. For example, we know that the width of the Fermi PSF is about 12 +/- 3 pixels, and the background in this part of the sky is about 0.5 counts/pixel.\n",
    "\n",
    "However, because our information on this source comes from LIGO, we don't really know anything about the brightness (i.e. amplitude) of the source in gamma-rays.\n",
    "\n",
    "**Exercise**: Can we incorporate this information as a prior? What kind of shape would you choose for your prior? (Solution below).\n",
    "\n",
    "\n",
    "**Hint**: `scipy.stats` has a wealth of distributions that are useful here. For example, to define a Gaussian prior with a mean of 2 and a variance of 1, we can use `scipy.stats.norm(2,1).logpdf(x)` to calculate the log-probability of value `x` given that normal prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-evolution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-region",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-proposition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-february",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-discount",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "painful-sphere",
   "metadata": {},
   "source": [
    "**Solution**: \n",
    "\n",
    "This is what's called an **informative prior**. Informative priors are those where we have meaningful information prior to looking at the data to help us constrain the problem. Note that priors, in general \n",
    "\n",
    "Let's assume that LIGO told us their positional probabilities are approximately Gaussian, so we can pick a Gaussian prior for the positional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos1_prior = scipy.stats.norm(150, 20).logpdf\n",
    "pos2_prior = scipy.stats.norm(180, 100).logpdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-funeral",
   "metadata": {},
   "source": [
    "We unfortunately don't know much about the amplitude in advance. For this parameter, we'll need what's called an **uninformative prior**. These are priors that encode our lack of information. Naively, you might expect that an uninformative prior would assign the same probability to each possible value across a range. However, for some parameters, this turns out not to be a good, uninformative choice. This is especially true for parameters that can range over multiple orders of magnitude. The problem here is that when you assign the same probability to all values, then the probability of a value between 0.1 and 0.2 is *much* smaller than the probability of a value between 1 and 2. To avoid this, we use what's called a *Jeffrey's prior*, which assigns equal probability to the *logarithm of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "logamp_prior = scipy.stats.uniform(-2, 15).logpdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-taste",
   "metadata": {},
   "source": [
    "Finally, we also need a prior for the width. For this parameter, let's assume we know from the Fermi documentation that the PSF is about 12 +/- 3 pixels. Again, we'll translate this to a Gaussian prior.\n",
    "\n",
    "The prior for the background parameter is also Gaussian, and based on our (presumed) knowledge of the gamma-ray sky. For this, we have a prior estimate of 0.5 +/- 0.1.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Write down the log-priors for the width and the background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-lodge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modern-stephen",
   "metadata": {},
   "source": [
    "Note that in principle, we could construct more complicated priors. For example, imagine that we know that the width of the PSF depends on the incoming flux (so on our amplitude parameter). We could construct a prior where the width prior actually depends on the amplitude parameter. \n",
    "\n",
    "However, today we'll forge ahead with the priors we have. According to Bayes' rule, the prior probability density for all parameters is the product of the individual densities. Because we've designed them as logarithms above, this then becomes a sum.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Complete the function below to construct the prior of the whole set of parameters.\n",
    "Make sure to account for the fact that the amplitude prior is defined on the *logarithm* of the amplitude parameter, not on the parameter itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(pars):\n",
    "    amp = pars[0]\n",
    "    pos1 = pars[1]\n",
    "    pos2 = pars[2]\n",
    "    width = pars[3]\n",
    "    bkg = pars[4]\n",
    "    \n",
    "    lp = pos1_prior(pos1) + # complete this function\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-highway",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-detector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "settled-letters",
   "metadata": {},
   "source": [
    "Let's compute the prior for the starting parameters you defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "logprior(start_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-ordinance",
   "metadata": {},
   "source": [
    "## The Posterior\n",
    "\n",
    "We are now set up to calculate the posterior probability density. The posterior density is essentially the product of the likelihood and prior (except for a normalization factor, which is important in the context of model comparison, but which we can safely neglect for parameter estimation). \n",
    "\n",
    "Again, defining our quantities in terms of their logarithm means that we can add them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logposterior(pars, x, y, poisson_flux):\n",
    "    lpost = loglikelihood(pars, x, y, poisson_flux) + logprior(pars)\n",
    "    \n",
    "    if np.isnan(lpost):\n",
    "        lpost = -logmin\n",
    "    elif lpost == np.inf:\n",
    "        lpost = -logmin\n",
    "        \n",
    "    return lpost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "logposterior(test_pars, x, y, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "logposterior(start_pars, x, y, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-thompson",
   "metadata": {},
   "source": [
    "## Estimating the Posterior\n",
    "\n",
    "In only very rare special cases, the posterior probability density is analytically tractable, that is, you can write down a functional form for it. This is particularly the case if you have defined what are called *conjugate* priors, which are priors that specifically make the posterior analytically tractable. Being able to cast your priors into a conjugate form is always an advantage, but rare in practice, especially when your problem has many parameters. You can find conjugate priors for commong likelihoods in various forms online. \n",
    "\n",
    "Alternatively, you can do optimization on the posterior the same way that we've done on the likelihood. This is called *maximum a posteriori estimation* and allows you to get quick parameter estimates that take our prior knowledge into account.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Write down the code to do a maximum-a-posteriori estimation in the same way we've done above with the maximum likelihood estimation, using `scipy.optimize.minimize`. \n",
    "\n",
    "\n",
    "**Optional Exercise**: Start from different positions, and see how this changes the result. What happens if you pick starting values outside of the support of the prior (e.g. out of the bounds of flat priors, or very far out in the wings of a Gaussian prior)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-poster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-infrared",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-board",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "novel-norwegian",
   "metadata": {},
   "source": [
    "## Markov Chain Monte Carlo\n",
    "\n",
    "However, there's a different way to characterize the posterior probability density. One advantage of writing down a posterior is that it gives us probability distributions over *parameters*, which we're interested in, especially if these distributions are not as well-behaved as required by typical optimization to give meaningful estimates.\n",
    "\n",
    "Again, we could generate a grid and then plot the results. However, the same problems surrounding how to pick the grid size and computational inefficiencies still apply. \n",
    "\n",
    "Instead, what we want is an algorithm that explores parameter space relatively efficiently, but is also not concerned *just* with finding the maximum value, but rather can give us a fuller description of the probability distribution. \n",
    "\n",
    "Enter Markov Chain Monte Carlo! A full tutorial on MCMC is beyond the scope of this set of exercises, so if you're interested in more details, take a look at [this paper](https://iopscience.iop.org/article/10.3847/1538-4365/aab76e/meta). For the purposes of today, here's what you need to know.\n",
    "\n",
    "MCMC describes a set of algorithms that define rules to intelligently step in parameter space, with the goal to explore the probability distribution. In the basic concept, starting from some point in parameter space, the algorithm will pick a new point according to some proposal distribution $q(\\theta)$. It then compares the posterior probability of the new point with the posterior probability of the old point. If the ratio of the two $r$ is larger than one, the new point is kept. If it's smaller than 1, then the new point will be accepted with probability $r$ and rejected with probability $1-r$. If it's not accepted, the original point will be duplicated, and the procedure starts again. If the new point is accepted, it will become the next point to jump from according to the proposal distribution $q$. In this way, we build up a list of points--or more commonly referred to as *samples*--that is denser in regions of high probability and less dense in regions of low probability. If you run this algorithm for long enough (ideally, to infinity, but who has time for that?), the samples will provide a good approximation to the posterior density. \n",
    "\n",
    "The most well-known MCMC algorithm is the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolisâ€“Hastings_algorithm). There are also a number of related methods like Nested Sampling, Slice Sampling and Hamiltonian Monte Carlo, which all approach the sampling problem somewhat differently.  \n",
    "\n",
    "**Exercise for at home**\n",
    "\n",
    "Write your own Metropolis-Hastings sampler! There are better algorithms for most problems (though don't underestimate how powerful Metropolis-Hastings can be on the right problem!), but writing your own sampler is invaluable for understanding some of the tricky problems with samplings. I encourage you to write your own, and compare it to samplers available in existing software packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-canvas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-plate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-drunk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-brake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "electrical-holder",
   "metadata": {},
   "source": [
    "We're not going to write our own sampler here, but rather use an implementation in the popular package `emcee`. If you don't have that package installed, you can do so by typing `pip install emcee` into the empty cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-spain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "upper-mortgage",
   "metadata": {},
   "source": [
    "`emcee` works somewhat similar to the Metropolis-Hastings algorithm outlined below, but includes some important improvements. It also doesn't run a single sampling process as we've outlined above, but rather many of them simultaneously. Let's set this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parallel sampling processes\n",
    "nwalkers = 32\n",
    "\n",
    "### number of dimensions for the Gaussian seeds (= number of parameters)\n",
    "ndim = len(res.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-specification",
   "metadata": {},
   "source": [
    "We need to sample some random starting positions. We're going to do that using the best-fit optimization result above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample random starting positions for each of the walkers\n",
    "p0 = [np.random.multivariate_normal(res.x,res.hess_inv) for i in range(nwalkers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialize sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior, args=[x, y, counts])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-cleveland",
   "metadata": {},
   "source": [
    "While we made a good guess that the starting parameters are near the true properties of our source, we don't know that in practice. So initially, it's possible that our sampling processes will be far away from a region of high probability. While MCMC algorithms are designed such that they will (ideally) eventually move to regions of high probability, they might initially not be. This means that the first steps in our process will not accurately reflect the true posterior distribution.\n",
    "\n",
    "For this reason, we generally throw away the initial $k$ steps in our chain, which is called \"burn-in\". After that, we hope that the chain is *burned-in*, that is, it has moved to our posterior distribution. Let's try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-words",
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin_samples = 1000\n",
    "pos, prob, state = sampler.run_mcmc(p0, burnin_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-circulation",
   "metadata": {},
   "source": [
    "This might take a few minutes to run, depending on your computer!\n",
    "\n",
    "One useful check is to plot the all the samples from all the processes. If there are large trends in the samples over time, that is, if the processes wander about, then the processes is likely not burned in yet, and we need to run it for longer. If the chains look like random noise, they might be burned in and good to go.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Plot the chains for each parameter. Are they stable around some value? Do they wander about in parameter space? \n",
    "\n",
    "Hint: You can get the samples using the command `sampler.get_chain()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-professor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-walker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-advertising",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tired-miami",
   "metadata": {},
   "source": [
    "Let's run this for longer, and then throw away the first thousand or so samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_ = sampler.run_mcmc(pos, 1000, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(8,6))\n",
    "\n",
    "axes = np.hstack(axes)\n",
    "\n",
    "for i in range(len(res.x)):\n",
    "    axes[i].plot(samples[:,:,i], alpha=0.1, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-kuwait",
   "metadata": {},
   "source": [
    "There are other useful ways to check how well your chains are converged to what's hopefully the true posterior distribution (check out the reference at the start of this section for other approaches), but it's worth keeping in mind that truly knowing that this is the case is pretty much impossible in many practical applications. Depending on the structure of the posterior probability, and how and where you initialize your chains, you might entirely miss an important region of high-probability, and never know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.get_chain(discard=1000, flat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-investigation",
   "metadata": {},
   "source": [
    "A useful way to visualize your MCMC chains is using what's called a *corner plot*. This takes your multi-dimensional distribution, and plots each dimension as a histogram, and each pair of dimensions as a contour plot. \n",
    "\n",
    "You can write the code to do this yourself (it's not hard), or you can use the package `corner` (install via `pip install corner`) as a convenient visualization tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposite-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-southeast",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corner.corner(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-translator",
   "metadata": {},
   "source": [
    "This posterior looks pretty regular and symmetric, which is nice! You can, of course, summarize this into typical mean and uncertainty estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = [\"amplitude\", \"x coord\", \"y coord\", \"width\", \"bkg\"]\n",
    "\n",
    "for i in range(len(res.x)):\n",
    "    print(\"%s posterior: \"%parameter_names[i] + str(np.mean(samples[:,i])) + \" +/- \" + str(np.std(samples[:,i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-suspension",
   "metadata": {},
   "source": [
    "For problems where posteriors are asymmetric or have multiple peaks (also called *modes*), you might prefer the median and quantiles to summarize the distribution, or simply use the corner plot above as your representation of the posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-annex",
   "metadata": {},
   "source": [
    "## Exploring (Posterior) Inference\n",
    "\n",
    "We're not done just having found a maximum likelihood value or a posterior sample. We now have to answer the most important question: **Does our result make sense**? \n",
    "\n",
    "This is, of course, a complicated question to answer, and it's very dependent on the problem you're solving. \n",
    "But here are some starting questions you can ask yourselves:\n",
    "* Is the likelihood function I've chosen actually appropriate for the data you have? Or did my data come from a different distribution? Was your collaborator right? Should you use a Gaussian likelihood here? Or do you think you should use a Poisson distribution? Why or why not?\n",
    "* If I take my best-fit model, or draw samples from the posterior, are the functions it generates good representations of the data I've observed? Hint: This is hard to do in 2D, so I'd recomment plotting the data and model for slices through the image.\n",
    "* Is my MCMC procedure converged to the posterior?\n",
    "* Do my priors make sense? Especially hard boundaries on flat priors are a very strong assumption (you're saying there's no way the parameter value could ever lie outside of those boundaries), so checking whether these make sense can be a useful way to troubleshoot.\n",
    "* Where are there bugs in my code? Because there are always bugs in code. :) \n",
    "* What does my posterior look like compared to my prior? Are they close to each other? Are they far away from each other? \n",
    "\n",
    "**Useful Information**: If you've done model fitting in the past, you might have heard that the summed squared residuals, or $\\chi^2$ divided by the degrees of freedom, should be close to 1 for the model to be a good representation. This works because for data with Gaussian uncertainties (where you use a Gaussian likelihood in your modeling), the sum of the log-likelihood for all data points is distributed following a $\\chi^2$ distribution scaled by the degrees of freedom, which in turn means that if you divide by the degrees of freedom, you get a central chi-square distribution with a mean of $1$. The further away you are from that mean, the smaller the probability that your residuals were independent draws from a Gaussian distribution, which they should be if the model were exactly the true, underlying process that generated your observed data. This is a useful diagnostic for checking your model, but it only works well if your likelihood is Gaussian.\n",
    "\n",
    "**Exercise 1**:\n",
    "\n",
    "Do you think that your posterior inference gets close to the true parameters? Why or why not? \n",
    "\n",
    "**Exercise 2**:\n",
    "\n",
    "Compare your results with some of the other groups. Are they similar? Are they very different? If they are different, can you figure out why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-mason",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
